# ğŸ¯ EVERYTHING YOU NEED TO KNOW

## TL;DR - Just Tell Me What To Do!

### **If you have a good GPU (RTX 3090+):**
```bash
cd "/Users/arunmoorthy/Junior Year/CS 229/Final Project"
./setup_local.sh
# Download datasets (links in output)
./preprocess_data.sh
python3 main.py --stage direct --config config.yaml
```

### **If you want to use cloud GPU (Modal):**
```bash
cd "/Users/arunmoorthy/Junior Year/CS 229/Final Project"
./setup_local.sh
# Download datasets (links in output)
./preprocess_data.sh
./setup_modal.sh
modal run modal_app.py --command train --stage direct
```

---

## ğŸ“Š WHERE IS THE DATA?

### You Need To Download These:

#### 1. **SemanticKITTI** (REQUIRED - Real LiDAR scans)
- **Size:** 80 GB (velodyne) + 200 MB (labels)
- **Link:** http://semantic-kitti.org/
- **Steps:**
  1. Go to website
  2. Register (academic email helps)
  3. Agree to terms
  4. Download:
     - `data_odometry_velodyne.zip` (80 GB)
     - `data_odometry_labels.zip` (200 MB)
  5. Extract both to: `data/raw/SemanticKITTI/`

**Final structure should be:**
```
data/raw/SemanticKITTI/
â””â”€â”€ sequences/
    â”œâ”€â”€ 00/
    â”‚   â”œâ”€â”€ velodyne/
    â”‚   â”‚   â”œâ”€â”€ 000000.bin
    â”‚   â”‚   â”œâ”€â”€ 000001.bin
    â”‚   â”‚   â””â”€â”€ ... (4,500+ files)
    â”‚   â””â”€â”€ labels/
    â”‚       â”œâ”€â”€ 000000.label
    â”‚       â””â”€â”€ ...
    â”œâ”€â”€ 01/
    â”œâ”€â”€ 02/
    â””â”€â”€ ... (sequences 00-21)
```

#### 2. **SynLiDAR** (REQUIRED - Synthetic LiDAR scans)
- **Size:** 20-40 GB
- **Link:** https://github.com/xiaoaoran/SynLiDAR
- **Steps:**
  1. Go to GitHub repo
  2. Find download link (usually Google Drive)
  3. Download the dataset
  4. Extract to: `data/raw/SynLiDAR/`

**Or use CARLA simulator:**
- Install CARLA: https://carla.org/
- Generate your own synthetic scans
- Save as .bin files (KITTI format)

---

## ğŸ”§ DO I NEED MODAL?

### **NO! You have two options:**

| Feature | Local Training | Modal (Cloud) |
|---------|---------------|---------------|
| **GPU Required** | Yes (RTX 3090+) | No |
| **Cost** | $0 (use your GPU) | ~$50-70 |
| **Speed** | 8-12 hours (Stage A) | 6 hours (Stage A) |
| **Setup** | Simpler | Need Modal account |
| **Best for** | You have good GPU | No GPU or want A100 |

### **My Recommendation:**

- **Have RTX 3090/4090/A6000?** â†’ Train locally (free!)
- **Have RTX 3060/3070 or worse?** â†’ Use Modal
- **No GPU at all?** â†’ Definitely use Modal

---

## ğŸš€ HOW TO START TRAINING

### **Complete Step-by-Step (Local)**

#### **Step 1: Initial Setup** (5 minutes)
```bash
cd "/Users/arunmoorthy/Junior Year/CS 229/Final Project"

# Install dependencies
pip3 install -r requirements.txt

# Test installation
python3 test_installation.py
```

**Expected:** All tests pass âœ“

#### **Step 2: Download Data** (2-4 hours - depends on internet)

1. **Download SemanticKITTI:**
   - Go to: http://semantic-kitti.org/
   - Download velodyne (80GB) + labels (200MB)
   - Extract to: `data/raw/SemanticKITTI/`

2. **Download SynLiDAR:**
   - Go to: https://github.com/xiaoaoran/SynLiDAR
   - Follow their download link
   - Extract to: `data/raw/SynLiDAR/`

3. **Verify:**
```bash
ls data/raw/SemanticKITTI/sequences/00/velodyne/*.bin
ls data/raw/SynLiDAR/
```

#### **Step 3: Preprocess** (1-2 hours)
```bash
# This converts 3D point clouds â†’ 2D range images
./preprocess_data.sh
```

**What happens:**
- Projects point clouds to range view (64Ã—1024 images)
- Learns sensor characteristics from real data
- Creates `sensor_profile.json`

**Output:**
- `data/processed/semantickitti/` - ~23K preprocessed scans
- `data/processed/synlidar/` - preprocessed synthetic scans
- `sensor_profile.json` - learned real sensor characteristics

#### **Step 4: Train** (8-12 hours on RTX 3090/4090)
```bash
# Start training
python3 main.py --stage direct --config config.yaml

# In another terminal, monitor progress:
tensorboard --logdir ./outputs
# Open: http://localhost:6006
```

**What happens:**
- Trains UNet to translate synthetic â†’ realistic
- Saves checkpoints every 5 epochs
- Best model saved as `outputs/direct/checkpoints/best.pt`

**Expected outputs:**
- `outputs/direct/checkpoints/best.pt` - Best model
- `outputs/direct/logs/` - TensorBoard logs
- Training loss should decrease steadily

#### **Step 5: Evaluate** (2 hours)
```bash
# Translate synthetic validation set
python3 scripts/translate_batch.py \
  --checkpoint ./outputs/direct/checkpoints/best.pt \
  --input_dir ./data/processed/synlidar/val \
  --output_dir ./outputs/translated \
  --batch_size 16

# Evaluate with downstream task
python3 scripts/eval_downstream.py \
  --config config.yaml \
  --all \
  --synthetic_dir ./outputs/translated
```

**Expected results:**
- FRID: 50-150 (lower is better)
- mIoU improvement: +2-5%

---

### **Complete Step-by-Step (Modal)**

#### **Step 1-3: Same as Local**
Follow steps 1-3 from local setup (install, download, preprocess)

**Why?** Preprocessing is faster locally, then upload to Modal

#### **Step 4: Setup Modal** (10 minutes)
```bash
# Install Modal
pip3 install modal

# Create account & authenticate (opens browser)
modal token new

# Create storage volumes
modal volume create lidar-data
modal volume create lidar-checkpoints
modal volume create lidar-eval

# Upload preprocessed data (takes 20-30 min)
modal volume put lidar-data data/processed/semantickitti /data/SemanticKITTI
modal volume put lidar-data data/processed/synlidar /data/SynLiDAR
modal volume put lidar-data sensor_profile.json /data/sensor_profile.json
```

Or use the script:
```bash
./setup_modal.sh  # Does all the above
```

#### **Step 5: Train on Modal** (6 hours on A100)
```bash
# Start training on cloud GPU
modal run modal_app.py --command train --stage direct
```

**Monitor at:** https://modal.com/apps

**What happens:**
- Spins up A100-80GB GPU
- Loads your data from volumes
- Trains model
- Saves checkpoints back to volume
- Shuts down when done

**Cost:** ~$15-20 (6 hours Ã— $2.50/hr)

#### **Step 6: Download & Evaluate** (2 hours)
```bash
# Download trained model
modal volume get lidar-checkpoints /checkpoints/direct/checkpoints/best.pt ./outputs/

# Translate locally (or on Modal)
python3 scripts/translate_batch.py \
  --checkpoint ./outputs/best.pt \
  --input_dir ./data/processed/synlidar/val \
  --output_dir ./outputs/translated

# Evaluate
python3 scripts/eval_downstream.py --all --synthetic_dir ./outputs/translated
```

---

## ğŸ’° COST BREAKDOWN

### Local Training
- Hardware needed: RTX 3090+ GPU (32GB+ RAM)
- Electricity: ~$5-10 for 30 hours
- **Total: $5-10**

### Modal Training
- A100-80GB: $2.50/hour
- Stage A training: 6 hours = $15
- Stage B (optional): 12 hours = $30
- Evaluation: 2 hours = $5
- **Total: $50-70**

---

## ğŸ“ WHAT FILES DO WHAT?

### **Files You'll Use:**

| File | What It Does | When To Use |
|------|--------------|-------------|
| `START_HERE.md` | Quick start guide | Read first! |
| `SETUP_GUIDE.md` | Detailed setup instructions | Reference |
| `config.yaml` | All settings | Edit to tune model |
| `main.py` | Training script | Run to train |
| `test_installation.py` | Verify setup | After install |

### **Scripts You'll Run:**

| Script | Purpose | Time |
|--------|---------|------|
| `setup_local.sh` | Setup environment | 5 min |
| `preprocess_data.sh` | Preprocess datasets | 1-2 hrs |
| `setup_modal.sh` | Setup Modal cloud | 30 min |
| `main.py` | Train model | 6-12 hrs |
| `scripts/translate_batch.py` | Generate translations | 1 hr |
| `scripts/eval_downstream.py` | Evaluate results | 2 hrs |

### **Directories:**

```
Your Project/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # Download datasets here
â”‚   â”‚   â”œâ”€â”€ SemanticKITTI/
â”‚   â”‚   â””â”€â”€ SynLiDAR/
â”‚   â””â”€â”€ processed/        # Created by preprocessing
â”‚       â”œâ”€â”€ semantickitti/
â”‚       â””â”€â”€ synlidar/
â”‚
â”œâ”€â”€ outputs/              # Training outputs
â”‚   â”œâ”€â”€ direct/
â”‚   â”‚   â”œâ”€â”€ checkpoints/  # Model checkpoints
â”‚   â”‚   â””â”€â”€ logs/         # TensorBoard logs
â”‚   â””â”€â”€ translated/       # Translated scans
â”‚
â”œâ”€â”€ models/               # Model architectures (code)
â”œâ”€â”€ data/                 # Data loading (code)
â”œâ”€â”€ train/                # Training (code)
â”œâ”€â”€ eval/                 # Evaluation (code)
â””â”€â”€ scripts/              # Utility scripts
```

---

## âš™ï¸ CONFIGURATION

All settings are in `config.yaml`. Key ones:

### **For GPU memory issues:**
```yaml
training:
  batch_size: 8  # Reduce from 16
sensor:
  n_azimuth: 512  # Reduce from 1024
```

### **For faster training (lower quality):**
```yaml
training:
  num_epochs: 50  # Reduce from 100
model:
  base_channels: 32  # Reduce from 64
```

### **For better quality (slower):**
```yaml
training:
  num_epochs: 150  # Increase
model:
  base_channels: 128  # Increase
  channel_multipliers: [1, 2, 4, 8, 16]  # Add more
```

---

## ğŸ› COMMON ISSUES

### "python: command not found"
â†’ Try `python3` instead

### "FileNotFoundError: data/raw/SemanticKITTI"
â†’ Download the data first (see "WHERE IS THE DATA?" above)

### "CUDA out of memory"
â†’ Reduce `batch_size` in config.yaml to 8 or 4

### "Modal authentication failed"
â†’ Run `modal token new`

### "Preprocessing takes forever"
â†’ Normal! 23K scans take time. Go get coffee â˜•

### Training loss is NaN
â†’ Reduce learning rate in config.yaml to 0.0001

---

## ğŸ“Š WHAT TO EXPECT

### During Training:
- **First hour:** Loss drops fast (80 â†’ 20)
- **Hours 2-4:** Gradual improvement (20 â†’ 5)
- **Hours 4+:** Fine-tuning (5 â†’ 2)

### After Training:
- **FRID:** Should be 30-50% lower than raw synthetic
- **mIoU:** Should improve by 2-5%
- **Visual quality:** Translated scans look more realistic

### Checkpoints:
- Saved every 5 epochs
- Best model saved separately
- Can resume if training crashes

---

## âœ… QUICK CHECKLIST

Before starting:
- [ ] Python 3.10+ installed
- [ ] GPU with 16GB+ VRAM (or Modal account)
- [ ] 200+ GB free disk space
- [ ] Stable internet for downloads

After setup:
- [ ] `test_installation.py` passes
- [ ] Downloaded SemanticKITTI
- [ ] Downloaded SynLiDAR
- [ ] Ran preprocessing
- [ ] `sensor_profile.json` exists

During training:
- [ ] Loss decreasing
- [ ] Checkpoints being saved
- [ ] TensorBoard shows curves

After training:
- [ ] `best.pt` checkpoint exists
- [ ] Translated scans generated
- [ ] Metrics computed
- [ ] Downstream eval done

---

## ğŸ“ FOR YOUR CS 229 PROJECT

This gives you a complete project:

### âœ… **Implementation:**
- Novel approach (range-view diffusion for LiDAR)
- Complete codebase (~5000 lines)
- Well-documented

### âœ… **Experiments:**
- Baseline comparison (raw vs translated)
- Downstream evaluation (segmentation)
- Ablation studies (config changes)

### âœ… **Results:**
- Distribution metrics (FRID, MMD)
- Task performance (mIoU)
- Visualizations

### âœ… **Reproducibility:**
- All code provided
- Clear documentation
- Configuration files
- <$100 to reproduce

---

## ğŸ†˜ NEED HELP?

1. **Read these in order:**
   - START_HERE.md (you are here!)
   - SETUP_GUIDE.md (detailed instructions)
   - config.yaml (comments explain each setting)

2. **Check logs:**
   - Training: `./outputs/direct/logs/`
   - TensorBoard: `tensorboard --logdir ./outputs`

3. **Common issues:**
   - See "COMMON ISSUES" section above
   - Check SETUP_GUIDE.md troubleshooting

4. **Still stuck?**
   - Check file paths in config.yaml
   - Verify data downloaded correctly
   - Try reducing batch_size

---

## ğŸ¯ SUCCESS CRITERIA

Your project is working if:
- âœ… Training loss decreases to <5
- âœ… FRID is lower for translated vs raw
- âœ… mIoU improves with translated data
- âœ… Translated scans look more realistic

You'll get a good grade if:
- âœ… Complete implementation
- âœ… Rigorous evaluation
- âœ… Clear documentation
- âœ… Reproducible results

---

**You have everything you need. Now go get that data and start training! ğŸš€**

